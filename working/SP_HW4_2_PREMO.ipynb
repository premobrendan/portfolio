{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pylab inline\n",
        "import numpy.linalg as LA\n",
        "import codecs, json\n",
        "from time import time\n",
        "import plotly.express as px\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import collections\n",
        "import math\n",
        "from collections import Counter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZBQxNz7uoyr",
        "outputId": "6634f625-b1dd-43f0-ab5f-0edb0b52eb22"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/pylab.py:159: UserWarning: pylab import has clobbered these variables: ['random', 'time']\n",
            "`%matplotlib` prevents importing * from pylab and numpy\n",
            "  warn(\"pylab import has clobbered these variables: %s\"  % clobbered +\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\def\\m#1{\\mathbf{#1}}$\n",
        "$\\def\\mm#1{\\boldsymbol{#1}}$\n",
        "$\\def\\mb#1{\\mathbb{#1}}$\n",
        "$\\def\\c#1{\\mathcal{#1}}$\n",
        "$\\def\\mr#1{\\mathrm{#1}}$\n",
        "$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\brm{\\begin{rmat}}$\n",
        "$\\newcommand\\erm{\\end{rmat}}$\n",
        "$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n",
        "$\\newcommand\\bcm{\\begin{cmat}}$\n",
        "$\\newcommand\\ecm{\\end{cmat}}$\n"
      ],
      "metadata": {
        "id": "AJ3TyV41vA0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Q3: Natural Language Processing (****) (60pt)\n",
        "\n",
        "\n",
        "One of applications of Markov chain in natural language processing is language model. In particular, we will work on uni-gram and bi-gram.\n",
        "\n",
        "The following data set provides you with the training data files (a subset of the One Billion Word Language Modeling Bench-\n",
        "mark). Each line in each file contains a whitespace-tokenized sentence.\n",
        "\n",
        "- `1b benchmark.train.tokens`: data for training your language models.\n",
        "- `1b benchmark.dev.tokens`: data for debugging and choosing the best hyperparameters.\n",
        "- `1b benchmark.test.tokens`: data for evaluating your language models.\n",
        "\n",
        "\n",
        "I have precessed these dataset for the purposed of this problem. The total number of unique words is 80661. In bi-gram model, the size of the transition matrix is $80661\\times 80661$. It is impossible the store these matrices directly and we have to take advantage of the sparsity of the transition matrix.\n",
        "\n",
        "You should use the development data to choose the best values for the hyperparameters $\\lambda$. Hyperparameter\n",
        "optimization is an active area of research; for this homework, you can simply try a few combinations to find\n",
        "reasonable values."
      ],
      "metadata": {
        "id": "rDGj1uZgsPKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.train.tokens?raw=true -O 1b_benchmark.train.tokens\n",
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.dev.tokens?raw=true -O 1b_benchmark.dev.tokens\n",
        "!wget https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.test.tokens?raw=true -O 1b_benchmark.test.tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O23Noh1BuQBj",
        "outputId": "f04d73e9-ea0f-436c-e3ac-27013e7668bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-10 06:06:45--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.train.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8422714 (8.0M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.train.tokens’\n",
            "\n",
            "1b_benchmark.train. 100%[===================>]   8.03M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-11-10 06:06:47 (65.8 MB/s) - ‘1b_benchmark.train.tokens’ saved [8422714/8422714]\n",
            "\n",
            "--2023-11-10 06:06:47--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.dev.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1673607 (1.6M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.dev.tokens’\n",
            "\n",
            "1b_benchmark.dev.to 100%[===================>]   1.60M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2023-11-10 06:06:47 (18.6 MB/s) - ‘1b_benchmark.dev.tokens’ saved [1673607/1673607]\n",
            "\n",
            "--2023-11-10 06:06:47--  https://raw.githubusercontent.com/yexf308/AppliedStochasticProcess/main/HW/HW2/1b_benchmark.test.tokens?raw=true\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1650147 (1.6M) [text/plain]\n",
            "Saving to: ‘1b_benchmark.test.tokens’\n",
            "\n",
            "1b_benchmark.test.t 100%[===================>]   1.57M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-11-10 06:06:48 (18.6 MB/s) - ‘1b_benchmark.test.tokens’ saved [1650147/1650147]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = \"<s>\"\n",
        "STOP_TOKEN = \"</s>\"\n",
        "\n",
        "class FileParser:\n",
        "    def __init__(self, train_file=\"1b_benchmark.train.tokens\", test_file=\"1b_benchmark.test.tokens\", dev_file=\"1b_benchmark.dev.tokens\"):\n",
        "        self.TRAIN_FILE = train_file\n",
        "        self.TEST_FILE  = test_file\n",
        "        self.DEV_FILE = dev_file\n",
        "\n",
        "    def get_train_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TRAIN_FILE))\n",
        "\n",
        "    def get_dev_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.DEV_FILE))\n",
        "\n",
        "    def get_test_file_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TEST_FILE))\n",
        "\n",
        "    def get_train_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TRAIN_FILE), flatten=False)\n",
        "\n",
        "    def get_dev_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.DEV_FILE), flatten=False)\n",
        "\n",
        "    def get_test_file_sentence_tokens(self):\n",
        "        return self._tokenize(self._get_sentences(self.TEST_FILE), flatten=False)\n",
        "\n",
        "    def _flatten(self, l):\n",
        "        return [word for sublist in l for word in sublist]\n",
        "\n",
        "    def _tokenize(self, sentence_list, flatten=True):\n",
        "        tokenized_sentences = [re.split(\"\\s+\", sentence.strip()) for sentence in sentence_list]\n",
        "\n",
        "        if flatten:\n",
        "            return self._flatten(tokenized_sentences)\n",
        "\n",
        "        return tokenized_sentences\n",
        "\n",
        "    def _get_sentences(self, file_path):\n",
        "\n",
        "        l = []\n",
        "        with open(file_path, \"r\") as f:\n",
        "            l = f.readlines()\n",
        "\n",
        "        # Add the start and stop tokens to each sentence in the file\n",
        "        sentence_list = []\n",
        "        for sentence in l:\n",
        "            sentence_list.append(START_TOKEN + \" \" + sentence + \" \" + STOP_TOKEN)\n",
        "\n",
        "        return sentence_list"
      ],
      "metadata": {
        "id": "vCkmscSwua8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp = FileParser()\n",
        "train_tokens = fp.get_train_file_sentence_tokens()\n",
        "dev_tokens = fp.get_dev_file_sentence_tokens()\n",
        "test_tokens = fp.get_test_file_sentence_tokens()\n"
      ],
      "metadata": {
        "id": "KvOuK1qvufdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the dict and the map from the words to integers for the purpose of training.\n",
        "word2idx={}\n",
        "idx2word={}\n",
        "idx=0\n",
        "for line in train_tokens:\n",
        "    for word in line:\n",
        "        if word not in word2idx:\n",
        "            word2idx[word]=idx\n",
        "            idx2word[idx]=word\n",
        "            idx+=1\n",
        "\n",
        "# Building the dict and the map from the words to integers for dev.\n",
        "word2idx_dev={}\n",
        "idx2word_dev={}\n",
        "idx=0\n",
        "for line in dev_tokens:\n",
        "    for word in line:\n",
        "        if word not in word2idx_dev:\n",
        "            word2idx_dev[word]=idx\n",
        "            idx2word_dev[idx]=word\n",
        "            idx+=1\n",
        "\n",
        "# Transform the words in each sentence to integers\n",
        "train_data=[]\n",
        "for line in train_tokens:\n",
        "    train_data.append([word2idx[word] for word in line])\n",
        "\n",
        "dev_data=[]\n",
        "for line in dev_tokens:\n",
        "    dev_data.append([word2idx_dev[word] for word in line])\n",
        "\n"
      ],
      "metadata": {
        "id": "z5BB2o5Zuhka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Your task:** Complete the code in the ```LanguageModel``` class as follows.\n",
        "\n",
        "1. (15pt) Implement **Laplace Smoothing** with the smoothing factor =1 in the function ```get_unigram_probability``` and ```get_bigram_probability```. In the bigram model, try to implement it as efficient as possible. **Hint:** For bigrams $(t_1,t_2)$ which do not occur in the sample, what is $p(t_2|t_1)$? For fixed $t_1$, are these probability the same?\n",
        "\n",
        "2. (15pt) Complete functions ```get_unigram_sentence_log_probability``` and ```get_bigram_sentence_log_probability``` to calculate the log probability of the given sentence. You need to consider the situation that when Laplace Smoothing is true and the situation that when Laplace Smoothing is false.\n",
        "\n",
        "\n",
        "3. (10pt) To make your language model work better, you will implement linear interpolation smoothing between\n",
        "unigram, bigram.\n",
        "\\begin{align}\n",
        "p'(t_2|t_1) = \\lambda_1 p(t_2) + \\lambda_2 p(t_2|t_1)\n",
        "\\end{align}\n",
        "where $p'$ represents the smoothed probability, the hyperparameters $\\lambda_1, \\lambda_2$ are weights on the unigram,\n",
        "bigram language models, respectively. So $\\lambda_1+\\lambda_2= 1$.\n",
        "Complete functions ```get_linear_interpolation_probability``` and ```get_linear_interpolation_sentence_log_probability``` with $\\mm\\lambda = (\\lambda_1, \\lambda_2)$ stored in ```linear_interpolation_factors```.\n",
        "\n",
        "4. (5pt) Testing the smoothed probability with several sentences in the development dataset. You should test when laplace smoothing is True and when laplace smoothing is False. You might find some words in testing dataset are not appeared in training set and you can set a default probability for this situation.  \n",
        "\n",
        "3. (15pt) Report the **perplexity scores** of the linear interpolation of language model for your training,\n",
        "and development sets. Report no\n",
        "more than 5 different sets of $\\lambda$. Briefly discuss the experimental results. Putting it all together, report perplexity on the test set, using the hyperparameters that you chose from\n",
        "the development set. Specify those hyperparameters."
      ],
      "metadata": {
        "id": "DQ8WdtDDu1DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "\n",
        "    def __init__(self, laplace_smoothing=False, laplace_smoothing_factor=1):\n",
        "        self.unigram_freqs = {}\n",
        "        self.unigram_corpus_length = 0\n",
        "        self.num_unique_unigrams = 0\n",
        "\n",
        "        self.bigram_freqs = {}\n",
        "        self.bigram_corpus_length = 0\n",
        "        self.num_unique_bigrams = 0\n",
        "\n",
        "        self.laplace_smoothing = laplace_smoothing\n",
        "        self.laplace_smoothing_factor = laplace_smoothing_factor\n",
        "\n",
        "\n",
        "    def fit_unigram(self, tokens):\n",
        "        for clist in tokens:\n",
        "          for cindex in range(len(clist)):\n",
        "            t = clist[cindex]\n",
        "            self.unigram_freqs[(t)] = self.unigram_freqs.get((t),0) + 1\n",
        "\n",
        "        self.unigram_corpus_length = sum(list(self.unigram_freqs.values()))\n",
        "        self.num_unique_unigrams = len(list(self.unigram_freqs.keys()))\n",
        "\n",
        "\n",
        "    def fit_bigram(self, tokens):\n",
        "\n",
        "      for clist in tokens:\n",
        "        for cindex in range(0, len(clist) - 1):\n",
        "          t1, t2 =  clist[cindex], clist[cindex+1]\n",
        "          self.bigram_freqs[(t1, t2)] = self.bigram_freqs.get((t1, t2), 0) + 1\n",
        "\n",
        "      self.bigram_corpus_length = sum(list(self.bigram_freqs.values()))\n",
        "      self.num_unique_bigrams = len(list(self.bigram_freqs.keys()))\n",
        "\n",
        "\n",
        "    def get_unigram_probability(self, unigram):\n",
        "\n",
        "      if self.laplace_smoothing:\n",
        "        prob_numerator   = 1 + self.unigram_freqs.get(unigram, 0)\n",
        "        prob_denominator = self.num_unique_unigrams + self.unigram_corpus_length\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "      else:\n",
        "        prob_numerator   = self.unigram_freqs.get(unigram, 0)\n",
        "        prob_denominator = self.unigram_corpus_length\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def get_bigram_probability(self, bigram):\n",
        "      t1, t2 = bigram\n",
        "\n",
        "      if self.laplace_smoothing:\n",
        "        prob_numerator   = 1 + self.bigram_freqs.get((t1, t2), 0)\n",
        "        prob_denominator = self.num_unique_unigrams + self.unigram_freqs.get(t1, 0)\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "      else:\n",
        "        prob_numerator   = self.bigram_freqs.get((t1, t2), 0)\n",
        "        prob_denominator = self.unigram_freqs.get(t1, 0)\n",
        "        if prob_denominator == 0:\n",
        "            print(f\"Error: 'get_bigram_probability()' has a denominator of 0 for {bigram}\")\n",
        "            return float('inf')\n",
        "\n",
        "        prob = float(prob_numerator) / float(prob_denominator)\n",
        "        return prob\n",
        "\n",
        "\n",
        "    def get_linear_interpolation_probability(self, bigram, lif):\n",
        "      t1,t2 = bigram\n",
        "      lif1,lif2 = lif\n",
        "      value = (lif1*self.get_unigram_probability(t2))+(lif2*self.get_bigram_probability((t1,t2)))\n",
        "      return value\n",
        "\n",
        "\n",
        "    def get_unigram_sentence_log_probability(self, sentence):\n",
        "\n",
        "      sum=0\n",
        "      for x in sentence:\n",
        "        sum+=math.log(self.get_unigram_probability(x), 2)\n",
        "      return sum\n",
        "\n",
        "\n",
        "    def get_bigram_sentence_log_probability(self, sentence):\n",
        "\n",
        "      sum=0\n",
        "      for x1 in range(len(sentence)-1):\n",
        "        sum+=math.log(self.get_bigram_probability((sentence[x1],sentence[x1+1])), 2)\n",
        "      return sum\n",
        "\n",
        "\n",
        "    def get_linear_interpolation_sentence_log_probability(self, sentence, lif):\n",
        "\n",
        "      sum=0\n",
        "      for x1 in range(len(sentence)-1):\n",
        "        sum+=math.log(self.get_linear_interpolation_probability((sentence[x1],sentence[x1+1]), lif), 2)\n",
        "      return sum\n",
        "\n",
        "\n",
        "    def get_unigram_perplexity(self, sentences):\n",
        "\n",
        "      scores=[]\n",
        "      for x in sentences:\n",
        "        score=pow(2, (((-1)/len(x))*self.get_unigram_sentence_log_probability(x[1])))\n",
        "        scores.append([x[0], score])\n",
        "      return scores\n",
        "\n",
        "\n",
        "    def get_bigram_perplexity(self, sentences):\n",
        "\n",
        "      scores=[]\n",
        "      for x in sentences:\n",
        "        score=pow(2, (((-1)/len(x))*self.get_bigram_sentence_log_probability(x[1])))\n",
        "        scores.append([x[0], score])\n",
        "      return scores\n",
        "\n",
        "\n",
        "    def get_linear_interpolation_perplexity(self, sentences, lif):\n",
        "\n",
        "      scores=[]\n",
        "      for x in sentences:\n",
        "        score=pow(2, (((-1)/len(x))*self.get_linear_interpolation_sentence_log_probability(x[1], lif)))\n",
        "        scores.append([x[0], score])\n",
        "      return scores\n",
        "\n"
      ],
      "metadata": {
        "id": "lNsryribxCh4"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "lmf=LanguageModel()\n",
        "lmt=LanguageModel(laplace_smoothing=True)\n",
        "\n",
        "lmf.fit_unigram(dev_data)\n",
        "lmf.fit_bigram(dev_data)\n",
        "\n",
        "lmt.fit_unigram(dev_data)\n",
        "lmt.fit_bigram(dev_data)\n",
        "\n",
        "i=0\n",
        "tot_diff=0\n",
        "num_sentences = 500\n",
        "\n",
        "while i<num_sentences:\n",
        "  temp_index=random.randint(0,len(dev_data)-1)\n",
        "  temp_sentence = dev_data[temp_index]\n",
        "  difference=lmf.get_linear_interpolation_sentence_log_probability(temp_sentence, (.3,.7))- lmt.get_linear_interpolation_sentence_log_probability(temp_sentence, (.3,.7))\n",
        "  tot_diff+=difference\n",
        "  i+=1\n",
        "\n",
        "print(\"The average difference between no laplace smoothing and laplace smoothing in the log probability is\", tot_diff/num_sentences)\n",
        "# upon taking a difference between the two, we see that with laplace smoothing being true, we get a LOWER log probability in general for all sentences."
      ],
      "metadata": {
        "id": "JgpG50bbx9iT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27420d7f-7ee7-466c-e644-cf9df2dd8ba5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average difference between no laplace smoothing and laplace smoothing in the log probability is 125.06179490158868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm=LanguageModel(laplace_smoothing=True)\n",
        "\n",
        "lm.fit_unigram(train_data)\n",
        "lm.fit_bigram(train_data)\n",
        "\n",
        "i=0\n",
        "num_sentences = 5\n",
        "sentence_list = []\n",
        "\n",
        "while i<num_sentences:\n",
        "  temp_index=random.randint(0,len(dev_data)-1)\n",
        "  sentence_list.append([temp_index, dev_data[temp_index]])\n",
        "  i+=1\n",
        "\n",
        "print(lm.get_unigram_perplexity(sentence_list))\n",
        "print(lm.get_bigram_perplexity(sentence_list))\n",
        "print(lm.get_linear_interpolation_perplexity(sentence_list, (.49, .51)))\n",
        "print(lm.get_linear_interpolation_perplexity(sentence_list, (.3, .7)))\n",
        "print(lm.get_linear_interpolation_perplexity(sentence_list, (.7, .3)))\n",
        "print(lm.get_linear_interpolation_perplexity(sentence_list, (.1, .9)))\n",
        "print(lm.get_linear_interpolation_perplexity(sentence_list, (.9, .1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaMrmoiI3g2X",
        "outputId": "82c7cd20-78eb-4c1c-f8f7-45c735fc2176"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[11787, 2.4990909668821693e+33], [635, 7.079568752532694e+149], [10410, 5.4869755697143696e+53], [1371, 2.7793730533368023e+84], [5202, 5.599665377937189e+23]]\n",
            "[[11787, 3.80824022207682e+36], [635, 7.919038681593329e+169], [10410, 1.3567359281568313e+63], [1371, 5.882952711336944e+86], [5202, 4.928135215677749e+24]]\n",
            "[[11787, 1.9722815196886856e+32], [635, 5.823930127774991e+148], [10410, 1.3066027843997777e+53], [1371, 1.726917879165502e+81], [5202, 5.727970432496406e+22]]\n",
            "[[11787, 4.605252129296569e+32], [635, 2.3409743135225913e+151], [10410, 1.4116193720545303e+54], [1371, 3.5451181192633427e+81], [5202, 8.020462502452456e+22]]\n",
            "[[11787, 1.5537763163244355e+32], [635, 2.8775498844731327e+147], [10410, 3.8483179088682046e+52], [1371, 3.0935743955657546e+81], [5202, 5.654926129689887e+22]]\n",
            "[[11787, 5.05099289725323e+33], [635, 2.0277018635593446e+157], [10410, 4.321806127979578e+56], [1371, 6.989470518225716e+82], [5202, 2.339954155823195e+23]]\n",
            "[[11787, 2.3848911575016435e+32], [635, 5.816610651217776e+147], [10410, 4.129930081866037e+52], [1371, 3.2316172497604055e+82], [5202, 7.678911603129535e+22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4: (Bonus) Beam Search Revisited (*****) (15pt)\n",
        "Implement Beam Search Algorithm for Q3. Note you need to use various data structure and graphs to reduce the complexity of implementation.\n"
      ],
      "metadata": {
        "id": "vyaTm4mLbkiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code starts here.\n"
      ],
      "metadata": {
        "id": "vxRvZwfCcyUd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}