{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XUyVHRv2dnz9","outputId":"d2702722-bc3f-440b-c3b7-4dd03fd280a8","executionInfo":{"status":"ok","timestamp":1733902939109,"user_tz":300,"elapsed":7927,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Populating the interactive namespace from numpy and matplotlib\n"]}],"source":["%pylab inline\n","import pandas as pd\n","from scipy import linalg\n","from itertools import combinations\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import time\n","import scipy.io as io\n","from scipy.io import mmread\n","import scipy.sparse as sparse\n","from sklearn.tree import DecisionTreeClassifier\n"]},{"cell_type":"markdown","source":["$\\def\\m#1{\\mathbf{#1}}$\n","$\\def\\mm#1{\\boldsymbol{#1}}$\n","$\\def\\mb#1{\\mathbb{#1}}$\n","$\\def\\c#1{\\mathcal{#1}}$\n","$\\def\\mr#1{\\mathrm{#1}}$\n","$\\newenvironment{rmat}{\\left[\\begin{array}{rrrrrrrrrrrrr}}{\\end{array}\\right]}$\n","$\\newcommand\\brm{\\begin{rmat}}$\n","$\\newcommand\\erm{\\end{rmat}}$\n","$\\newenvironment{cmat}{\\left[\\begin{array}{ccccccccc}}{\\end{array}\\right]}$\n","$\\newcommand\\bcm{\\begin{cmat}}$\n","$\\newcommand\\ecm{\\end{cmat}}$"],"metadata":{"id":"gTyJG0bM5xVC"}},{"cell_type":"markdown","source":["# Homework 3\n","## Homework guideline\n","\n","- This is a group homework. Your group only needs to submit one homework. You can form a group of 1, 2 or 3.\n","\n","\n","- Submit your work(your reasoning and your code) as a SINGLE .ipynb document. Please rename the document as _HW1_YOURNAME.ipynb_ (for example, _HW1_FELIX.ipynb_). You are responsible for checking that you have correctly submitted the correct document. If your code cannot run, you may receive NO point.\n","\n","- Please justify all short answers with a brief explanation.\n","\n","- You only use the Python packages included in the following cell. You are not allowed to use other advanced package or modules unless you are permitted to.\n","\n","- In your final submission include the plots produced by the unedited code as presented below, as well as any additional plots produced after editing the code during the course of a problem. You may find it necessary to copy/paste relevant code into additional cells to accomplish this.\n","\n","- Feel free to use the lecture notes and other resources. But you\n","must understand, write, and hand in your own answers. In addition, you must write and submit\n","your own code in the programming part of the assignment (we may run your code).\n","If you copy someone else homework solution, both of you may receive ZERO point.\n","\n","- **Late assignments:** Recognizing that students may face unusual circumstances and require some flexibility in the course of the semester, each student will have a total of 7 free late (calendar) days to use as s/he sees fit, but no more than 4 late days can be used on any single assignment. Late days are counted at the granularity of days: e.g., 3 hours late is one late day.  \n","\n","- **You must write your own code and fill in the your answer in the text box.** If you fail to do either of that, you will receive zero point.\n","\n"],"metadata":{"id":"WJlRGSngU6Vz"}},{"cell_type":"markdown","source":["**Your group member:**"],"metadata":{"id":"kyph-tL466P4"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","#Q1:  Soft margin SVM (50pt)\n","\n","In the problem, you will implement the soft margin SVM using different gradient descent methods. Our\n","goal for this problem is to investigate the convergence of different gradient descent methods on a sample dataset and think about the characteristics of these different methods that lead to different performances.\n","\n","To recap, given a dataset of $n$\n","samples $\\mathcal{D} =\n","\\left\\{\\left(\\mathbf{x}^{(i)},\n","y^{(i)}\\right)\\right\\}_{i=1}^n$, where every $d$-dimensional feature vector $\\mathbf{x}^{(i)} \\in \\mathbb{R}^d$ is\n","associated with a label $y^{(i)} \\in\n","\\{-1,1\\}$, to estimate the parameters $\\boldsymbol{\\theta} = (\\mathbf{w}, b)$\n","of the soft margin SVM, we can minimize the loss function:\n","\n","\\begin{aligned}\n","f(\\mathbf{w},b; \\mathcal{D}) &= \\frac{1}{2}\n","\\|\\mathbf{w}\\|_2^2 + C \\sum_{(\\mathbf{x}^{(i)}, y^{(i)}) \\in\n","\\mathcal{D}} \\max\\left\\{0, 1 - y^{(i)}( \\mathbf{w}\\cdot\n","\\mathbf{x}^{(i)} + b )\\right\\} \\\\\n","&= \\frac{1}{2} \\|\\mathbf{w}\\|_2^2 + C\n","\\sum_{(\\mathbf{x}^{(i)}, y^{(i)}) \\in \\mathcal{D}}\n","L(\\mathbf{x}^{(i)}, y^{(i)}; \\boldsymbol{\\theta})\n","\\end{aligned}\n","\n","In order to minimize the function, we first obtain the gradient with\n","respect to $\\boldsymbol{\\theta}$. The\n","partial derivative with respect to $w_j$, the $j$-th entry in the vector $\\mathbf{w}$, is:\n","\n","\\begin{aligned}\n","    \\partial_{w_j} f(\\mathbf{w},b; \\mathcal{D}) =\n","    \\frac{\\partial f(\\mathbf{w},b; \\mathcal{D})}{\\partial w_j} =\n","    w_j + C \\sum_{(\\mathbf{x}^{(i)}, y^{(i)}) \\in \\mathcal{D}}\n","\\frac{\\partial L(\\mathbf{x}^{(i)}, y^{(i)};\n","\\boldsymbol{\\theta})}{\\partial w_j}\n","\\end{aligned}\n","where\n","\n","\\begin{aligned}\n","\\frac{\\partial L(\\mathbf{x}^{(i)}, y^{(i)};\n","\\boldsymbol{\\theta})}{\\partial w_j} =     \n","\\left\\{\\begin{array}{cl}\n","      0 & \\text{if}\\  y^{(i)}\\left(\\mathbf{w} \\cdot\n","\\mathbf{x}^{(i)} + b \\right) \\ge 1 \\\\\n","      -y^{(i)}x_j^{(i)} & \\text{otherwise.}\n","    \\end{array}\\right.\n","\\end{aligned}\n","and the partial derivative with respect to $b$ is\n","\\begin{aligned}\n","\\partial_b f(\\mathbf{w},b;\\mathcal{D})  =\n","\\frac{\\partial f(\\mathbf{w},b;\\mathcal{D})}{\\partial b} =\n","C \\sum_{(\\mathbf{x}^{(i)}, y^{(i)}) \\in \\mathcal{D}} \\frac{\\partial\n","L(\\mathbf{x}^{(i)}, y^{(i)}; \\boldsymbol{\\theta})}{\\partial b}\n","\\end{aligned}\n","where\n","\\begin{aligned}\n","\\frac{\\partial L(\\mathbf{x}^{(i)}, y^{(i)};\n","\\boldsymbol{\\theta})}{\\partial b} =     \n","\\left\\{\\begin{array}{cl}\n","      0 & \\text{if}\\  y^{(i)}\\left(\\mathbf{w} \\cdot\n","\\mathbf{x}^{(i)} + b \\right) \\ge 1 \\\\\n","      -y^{(i)} & \\text{otherwise.}\n","    \\end{array}\\right.\n","\\end{aligned}\n","\n","Since the direction of the gradient is the\n","direction of steepest ascent of the loss function, gradient descent\n","proceeds by iteratively taking small steps along the direction opposite\n","to the direction of gradient. The general framework of gradient descent\n","is given in following Algorithm.\n","\n","<img src=\"https://github.com/yexf308/AppliedStatistics/blob/main/Homework/HW2/algorithm.png?raw=true\" width=\"500\" />\n","\n","\n","**Task**:\n","\n","1. Implement the SVM algorithm using\n","the following gradient descent variants.\n","For all the variants use $C =\n","100, \\mathbf{w}^{(0)} =\n","\\mathbf{0}, b^{(0)} =\n","0$. For all other parameters, use the values specified in the\n","description of the variant.\n","\n","2. Run your implementation on the\n","data set. The data set contains the following\n","files:\n","- *features.txt*: Each\n","line contains the features (comma-separated values) of a single sample.\n","It has 6414 samples (rows) and 122 features (columns).\n","- *target.txt* : Each\n","line contains the target variable ($y = -1$ or $1$) for the corresponding\n","row in *features.txt*.\n","\n","3. Plot the value of the loss\n","function $f(\\mathbf{w}^{(t)},b^{(t)};\n","\\mathcal{D})$ vs. the iteration number $t$ starting from $t=0$. Label the plot axes. The diagram\n","should have graphs from all the three variants on the same plot. Report the total time (wall clock time, as opposed to the number of iterations) each of the gradient descent variants takes to converge. What do you infer from the plots and the time for convergence? Explain using 4-6\n","sentences.\n","\n","**Note**: update the parameters $\\mathbf{w}$ and $b$ on iteration $t$ using the values computed on iteration\n","$t-1$. Do not update using values\n","computed in the current iteration!"],"metadata":{"id":"pWZCnUue5N2M"}},{"cell_type":"code","source":["!wget https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW2/target.txt?raw=true -O target.txt\n","!wget https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW2/features.txt?raw=true -O features.txt\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pdo4kpjE5gzI","outputId":"9243eafe-090a-4438-b064-503f944d9023","executionInfo":{"status":"ok","timestamp":1733902944073,"user_tz":300,"elapsed":826,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-12-11 07:42:22--  https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW2/target.txt?raw=true\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17673 (17K) [text/plain]\n","Saving to: ‘target.txt’\n","\n","\rtarget.txt            0%[                    ]       0  --.-KB/s               \rtarget.txt          100%[===================>]  17.26K  --.-KB/s    in 0.001s  \n","\n","2024-12-11 07:42:22 (14.1 MB/s) - ‘target.txt’ saved [17673/17673]\n","\n","--2024-12-11 07:42:22--  https://raw.githubusercontent.com/yexf308/AppliedStatistics/main/Homework/HW2/features.txt?raw=true\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1565016 (1.5M) [text/plain]\n","Saving to: ‘features.txt’\n","\n","features.txt        100%[===================>]   1.49M  --.-KB/s    in 0.07s   \n","\n","2024-12-11 07:42:23 (22.9 MB/s) - ‘features.txt’ saved [1565016/1565016]\n","\n"]}]},{"cell_type":"code","source":["features = np.loadtxt('features.txt', delimiter=',')\n","target = np.loadtxt('target.txt', delimiter=',')\n","print(features.shape, target.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ri0K_7my5i-f","outputId":"af59e6ff-db58-41b7-dc9e-b36b2cc9768b","executionInfo":{"status":"ok","timestamp":1733902946254,"user_tz":300,"elapsed":125,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["(6414, 122) (6414,)\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Q1.1: Task 1&2:  Batch Gradient Descent (BGD) (15pt)\n","When the $\\beta = n$, in every iteration the\n","algorithm uses the entire dataset to compute the gradient and update the\n","parameters.\n","\n","As a convergence criterion for batch gradient\n","descent we will use $\\Delta_{\\% loss}^{(t)}\n","\\le \\varepsilon$, where\n","\\begin{aligned}\n","    \\Delta_{\\% loss }^{(t)} = \\frac{|f(\\mathbf{w}^{(t-1)},\n","b^{(t-1)}; \\mathcal{D}) - f(\\mathbf{w}^{(t)}, b^{(t)};\n","\\mathcal{D})|}{f(\\mathbf{w}^{(t-1)}, b^{(t-1)};\n","\\mathcal{D})}\\times100   \n","\\end{aligned}\n","\n","Set $\\eta = 3\\cdot10^{-7}$, $\\varepsilon = 0.25$.\n","\n","**Sanity Check 1**: The value of the loss function at iteration\n","number $t = 0$ must be around\n","641,400."],"metadata":{"id":"G53ky04J5o9j"}},{"cell_type":"code","source":["def Loss(x, y, w, b, C):\n","    # returns the value of the Loss function as defined above\n","    if type(y) == np.ndarray:\n","        sum = 0\n","        for i in range(len(y)):\n","            sum += max( 0, 1 - ( y[i] * (w.dot(x[i,:])+b) ) )\n","        return .5*(linalg.norm(w)**2) + C*sum\n","    else:\n","        return .5*linalg.norm(w)**2 + C*max(1 - ( y * (w.dot(x)+b) ), 0)\n","\n","def ConvergeCriterion(x, y, w, prev_w, b, prev_b, C):\n","    # returns the convergence criterion as defined above\n","    prev = Loss(x, y, prev_w, prev_b, C)\n","    curr = Loss(x, y, w, b, C)\n","    return (100*abs(prev - curr))/prev\n","\n","def DelF_w(x, y, w, b, C, j):\n","    # returns the value of the derivative of the loss function with respect to the jth value in w\n","    if type(y) == np.ndarray:\n","        sum = 0\n","        for i in range(len(y)):\n","            if y[i] * (w.dot(x[i,:])+b) < 1:\n","                sum+=(-1)*y[i]*x[i,j]\n","        return w[j] + C*sum\n","    else:\n","        if y * (w.dot(x)+b) < 1:\n","            return (-1)*y*x[j]\n","        else:\n","            return 0\n","\n","def DelF_b(x, y, w, b, C):\n","    # returns the value of the derivative of the loss function with respect to b\n","    if type(y) == np.ndarray:\n","        sum = 0\n","        for i in range(len(y)):\n","            if y[i] * (w.dot(x[i,:])+b) < 1:\n","                sum+= (-1)*y[i]\n","        return C*sum\n","    else:\n","        if y * (w.dot(x)+b) < 1:\n","            return (-1)*y\n","        else:\n","            return 0\n","\n","def unison_shuffled_copies(a, b):\n","    # returns a shuffled list of both x and y with respect to their indicies\n","    assert len(a) == len(b)\n","    p = np.random.permutation(len(a))\n","    return a[p], b[p]\n","\n","def GradientDescent(x, y, grad_type = \"batch\", learn_rate = .0001,\n","                           batch_size = 100, convergence_val = .25):\n","    \"\"\"\n","    This is the implementation of the above gradient descent algorithm, including\n","    both batch and stochastic type varients within.\n","\n","    INPUTS:\n","    x <- feature matrix\n","    y <- target vector\n","    grad_type <- type of Gradient Descent we wish to perform, either batch, mini batch or stochastic\n","    learn_rate <- affects how quickly our w and b variables change per iteration\n","    batch_size <- affects how much of the data set D we use\n","    convergence_val <- affects how large our Convergence Criterion must get before the algorithm ends\n","\n","    OUTPUTS:\n","    w <- weights vector, trained after convergence\n","    b <- coefficient, trained after convergence\n","    loss_list <- values of the loss function for each iteration\n","    \"\"\"\n","    N,d = x.shape\n","    assert batch_size <= N\n","    assert grad_type == \"batch\" or grad_type == \"stochastic\" or grad_type == \"mini batch\"\n","\n","    # initial conditions\n","    w = np.zeros((d,))\n","    b = 0\n","    C = 100\n","\n","    # shuffle x, y\n","    x,y = unison_shuffled_copies(x, y)\n","\n","    # Loss list\n","    loss_list = [Loss(x,y,w,b,C)]\n","\n","    # iteration\n","    t=0\n","    k=1\n","\n","    while t<10000: # used as a stop in case of divergence\n","        # iteration of t\n","        t+=1\n","\n","        # getting batch\n","        if batch_size == N:\n","            batch_x = x\n","            batch_y = y\n","        elif batch_size == 1:\n","            batch_x = x[k]\n","            batch_y = y[k]\n","        else:\n","            low = batch_size*k + 1\n","            high = min(batch_size*(k+1), N)\n","            batch_x = x[low:high]\n","            batch_y = y[low:high]\n","\n","        # learning w\n","        old_w = w.copy()\n","        old_b = b\n","        for j in range(0,d):\n","            w[j] = old_w[j] - (learn_rate*DelF_w(batch_x, batch_y, old_w, old_b, C, j))\n","        '''\n","        elif grad_type == \"stochastic\":\n","            old_w = w.copy()\n","            old_b = b\n","            if t > 1:\n","                old_old_w = old_w.copy()\n","                old_old_b = old_b\n","            for j in range(0,d):\n","                w[j] = old_w[j] - (learn_rate*DelF_w(batch_x, batch_y, old_w, old_b, C, j))\n","        elif grad_type == \"mini batch\":\n","            old_w = w.copy()\n","            old_b = b\n","            if t > 1:\n","                old_old_w = old_w.copy()\n","                old_old_b = old_b\n","            for j in range(0,d):\n","                w[j] = old_w[j] - (learn_rate*DelF_w(batch_x, batch_y, old_w, old_b, C, j))\n","        '''\n","\n","        # learning b\n","        b = old_b - (learn_rate*DelF_b(batch_x, batch_y, old_w, old_b, C))\n","\n","        # new k\n","        k = int((k+1)%np.ceil(N/batch_size))\n","\n","        # adding to Loss list\n","        loss_list.append(Loss(x,y,w,b,C))\n","\n","        # convergence criterion check\n","        if grad_type == \"batch\":\n","            conv = ConvergeCriterion(x, y, w, old_w, b, old_b, C)\n","            if conv <= convergence_val:\n","                print(\"job's done at t =\", t)\n","                break\n","        elif grad_type == \"stochastic\" or grad_type == \"mini batch\":\n","            if t == 1:\n","                conv_curr = ConvergeCriterion(x, y, w, old_w, b, old_b, C)\n","                conv_prev = 0\n","            else:\n","                conv_prev = conv\n","                conv_curr = ConvergeCriterion(x, y, w, old_w, b, old_b, C)\n","\n","            conv = .5*conv_curr + .5*conv_prev\n","            if conv < convergence_val:\n","                print(\"job's done at t =\", t)\n","                break\n","\n","    return w, b, loss_list\n"],"metadata":{"id":"azUTdQSX5t4J","executionInfo":{"status":"ok","timestamp":1733904934336,"user_tz":300,"elapsed":116,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["start_time = time.time()\n","\n","# i tuned convergence val to .2 so i get a slightly better curve on the plot\n","grad_w, grad_b, grad_loss = GradientDescent(features, target, grad_type = \"batch\",\n","                                            learn_rate = 3e-7, batch_size = 6414,\n","                                            convergence_val = .2)\n","\n","grad_time = time.time() - start_time\n","#print(\"General Gradient Descent b:\",grad_b)\n","#print(\"General Gradient Descent w:\",grad_w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVOoUua5ZnJn","executionInfo":{"status":"ok","timestamp":1733903052919,"user_tz":300,"elapsed":100639,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"0f4f9c20-724b-4706-b9ee-1a919a77159e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["job's done at t = 66\n"]}]},{"cell_type":"markdown","source":["\n","\n","\n","---\n","\n","\n","## Q1.2: Task 1&2: Stochastic Gradient Descent (SGD) (15pt)\n","When $\\beta = 1$, in every iteration the\n","algorithm uses one training sample at a time to compute the gradient and\n","update the parameters.\n","As a convergence criterion for stochastic\n","gradient descent we will use $\\Delta_{loss}^{(t)} < \\varepsilon$,\n","where \\begin{aligned}\n","    \\Delta_{loss}^{(t)} = \\tfrac{1}{2}\\Delta_{loss}^{(t-1)} +\n","\\tfrac{1}{2}\\Delta_{\\% loss}^{(t)},\n","\\end{aligned}\n","$t$ is the\n","iteration number, $\\Delta_{\\%\n","loss}^{(t)}$ is same as above and\n","and $\\Delta_{loss}^{(0)} = 0$.\n","Use $\\eta = 0.0001, \\varepsilon = 0.001$."],"metadata":{"id":"NI8qX8pI5wwG"}},{"cell_type":"code","source":["start_time = time.time()\n","\n","# not sure why im not getting to a clear convergence here, but i am for mini batch.\n","stoch_w, stoch_b, stoch_loss = GradientDescent(features, target, grad_type = \"stochastic\",\n","                                               learn_rate = .0001, batch_size=1,\n","                                               convergence_val = .001)\n","\n","stoch_time = time.time() - start_time\n","#print(\"Stochastic Gradient Descent b:\",stoch_b)\n","#print(\"Stochastic Gradient Descent w:\",stoch_w)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIMPVHa0n_Pb","executionInfo":{"status":"ok","timestamp":1733905060192,"user_tz":300,"elapsed":122971,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"dbc0f792-3972-4c35-a3bb-96e6135a0d98"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["job's done at t = 3170\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Q1.3: Task 1&2: Mini-Batch Gradient Descent (MBGD) (10pt)\n","In every iteration the algorithm uses mini-batches of $\\beta$ samples to compute the gradient and\n","update the parameters.\n","As a convergence criterion for mini-batch\n","gradient descent we will use $\\Delta_{loss}^{(t)} < \\varepsilon$,\n","where $\\Delta_{loss}^{(t)}$ is the\n","same as above  and $\\Delta_{loss}^{(0)} = 0$.\n","Use $\\eta = 10^{-5}, \\varepsilon = 0.01$ and $\\beta = 20$.\n","\n","**Sanity Check 2**: Batch GD should converge in 10-300\n","iterations and SGD between 500-3000 iterations with Mini Batch GD\n","somewhere in-between. However, the number of iterations may vary greatly\n","due to randomness. If your implementation consistently takes longer,\n","there might be a bug."],"metadata":{"id":"tg0bho9V56fX"}},{"cell_type":"code","source":["start_time = time.time()\n","\n","mb_w, mb_b, mb_loss = GradientDescent(features, target, grad_type = \"mini batch\",\n","                                      learn_rate = .00001, batch_size=20,\n","                                      convergence_val = .01)\n","\n","mb_time = time.time() - start_time\n","#print(\"Mini Batch Gradient Descent b:\",mb_b)\n","#print(\"Mini Batch Gradient Descent w:\",mb_w)"],"metadata":{"id":"MEX48vVVo9C4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733905117566,"user_tz":300,"elapsed":47913,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"8ad250d9-e4bf-4de1-e2f0-429c1ccdf851"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["job's done at t = 1079\n"]}]},{"cell_type":"markdown","source":["## Q1.4: Task 3: Plot (10pt)\n","Plot the value of the loss\n","function $f(\\mathbf{w}^{(t)},b^{(t)};\n","\\mathcal{D})$ vs. the iteration number $t$ starting from $t=0$. Label the plot axes. The diagram\n","should have graphs from all the three variants on the same plot. Report the total time (wall clock time, as opposed to the number of iterations) each of the gradient descent variants takes to converge. What do you infer from the plots and the time for convergence? Explain using 4-6\n","sentences.\n","\n","**Sanity Check 3**: The expected total run time for all 3\n","methods is around 5-15 minutes but might vary depending on the\n","implementation."],"metadata":{"id":"mqpxqz7U5_mL"}},{"cell_type":"code","source":["plt.plot(grad_loss)\n","plt.plot(stoch_loss)\n","plt.plot(mb_loss)\n","plt.xlabel(\"t\")\n","plt.ylabel(\"Loss\")\n","plt.title(\"Iterations vs. Loss Function and Runtime of Various Gradient Descent Methods\")\n","leg = [[\"batch\", round(grad_time,3)], [\"stochastic\", round(stoch_time,3)], [\"mini batch\", round(mb_time,3)]]\n","plt.legend(leg)"],"metadata":{"id":"DerYlCyO6Dti","colab":{"base_uri":"https://localhost:8080/","height":489},"executionInfo":{"status":"ok","timestamp":1733905122773,"user_tz":300,"elapsed":486,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"d7e262bd-d22f-4dba-c0b8-07481bfc2aae"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7f10fa53cac0>"]},"metadata":{},"execution_count":23},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# Your solution:\n","\n","First, it's important to be clear in regards to runtime vs iteration count. It is to be expected that the more information we use per iteration (i.e larger batch size), the fewer iterations it will take to get a result, at the cost of more computations per iteration. This clearly seems to balance out for the case of mini batch and stochastic (with the settings I used), but has a much bigger impact on batch, where iterations are very low but computation time is very high.\n","\n","As expected, the stochasitc plot has a very constant, linear downward direction towards its point of convergence, which we can infer would be around where the mini batch plot is near and where the batch plot stopped short of. This, I admit, is due to poor implimentation on my part, but I'm uncertain where I specifically went wrong. The other two plots are about what we expect: a sharp downward drop into a smoothing convergence, like what we see with the mini batch plot.\n"],"metadata":{"id":"DRnKNrKY6EHO"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","# Q2: Implementing Batch Naive Bayes (20pt)\n","\n","Implement a multinomial naive Bayes classifier in the `NaiveBayes` class.  Your implementation should support add-one smoothing. Whether or not to use add-one smoothing is controlled via an argument to the constructor; add-one smoothing is enabled by default.\n","\n","-  `__init(useAddOneSmoothing=True)__` : constructor.\n","\n","- `fit(X,Y)`: method to train the naive Bayes model.\n","\n","- `predict(X)`: method to use the trained naive Bayes model for prediction.\n","\n","- `predictProbs(X)`: outputs a matrix of predicted posterior class probabilities.\n","\n","\n","The training data for multinomial naive Bayes is specified as feature counts: `X[i,j]` is the number of times\n","feature $j$ occurs in instance $i$ (or you can think of it as that instance $i$ is characterized by a particular\n","real-valued amount of feature $j$).\n","\n","We are here using the multinomial distribution. Suppose our dataset has $D$ features and $K$ classes. Then the\n","multinomial distribution for a particular sample $(\\m{x},y)$, where $\\m{x}=[x_1,\\dots, x_D]\\in \\mb{R}^D$, $x_j\\in\\mb{N}$ for $j=1,\\dots, D$ and $y\\in \\{1,\\dots, K\\}$ is\n","\\begin{align}\n","p(\\m{x}|y) = \\frac{(\\sum_j x_j)! }{x_1! x_2! \\dots x_D!} p_{y1}^{x_1}p_{y2}^{x_2}\\dots p_{yD}^{x_D}\n","\\end{align}\n","and the label distribution $p(y)$ (in this case, a categorical distribution) is $\\mm\\pi=[\\pi_1, \\dots, \\pi_K]$.\n","\n","The naive Bayes assumption (that each instance is independent given the class label) is used in the sense that\n","that we are assuming the generative process is\n","\n","- Picking a class $y$ according to the label distribution\n","$p(y)$.\n","\n","- Generate a sequence of features, independently according to a multinomial distribution conditioned on the class $y$: $\\m{p}_y=(p_{y1}, p_{y2}, \\dots, p_{yD})$ with $\\sum_j p_{yj}=1$.\n","\n","What you are given is the count of each feature generated by this process stored in a vector $\\m{x}$. This is a useful model for predicting, say, document classes, where $d$ is size of vocabulary\n","and $K$ is number of document classes.\n","\n","The MLE parameter estimation for this naive Bayes probabilistic model is\n","- The label distribution $\\hat{p}(y)=\\hat{\\mm\\pi}$\n","\\begin{align}\n","\\hat{\\mm{\\pi}} = \\frac{N_c}{N}\n","\\end{align}\n","where $N_c$ is the number of samples with label $y=c$ and $N$ is the total samples.\n","\n","- The multinormial distribution with label $y=c$ is $\\hat{\\m{p}}_c=[\\hat{p}_{c1}, \\dots, \\hat{p}_{cD}]$ and\n","\\begin{align}\n","\\hat{p}_{cj}  =\\frac{N_{cj}}{N_c}\n","\\end{align}\n","where $N_{cj}$ is total occurrences of feature $j$ in samples with label $y=c$.\n","\n","\n","- When using add-one smoothing, we estimate $\\hat{p}_{cj} $ with\n","\\begin{align}\n","\\hat{p}_{cj}  = \\frac{N_{cj}+1}{N_c +D}\n","\\end{align}\n","\n","During prediction, given feature count vector $\\m{x}$, we estimate label $y$ posterior probability with\n","\\begin{align}\n","\\hat{p}(y|\\m{x})\\propto \\hat{p}(y)\\hat{p}_{y1}^{x_1}\\hat{p}_{y2}^{x_2}\\dots \\hat{p}_{yD}^{x_D}\n","\\end{align}\n","up to normalization. You might want to implement the equation above with summation of log probabilities\n","for better numerical stability. If you choose to do so, you would need another **numerical trick** below.\n","\n","After\n","obtaining the log probabilities for each classes, $\\m{z}=\\bcm\\log \\hat{p}(y=1|\\m{x}),\\log \\hat{p}(y=2|\\m{x}), \\dots, \\log \\hat{p}(y=K|\\m{x}) \\ecm$. The actual probability distribution to be output is\n","\\begin{align}\n","p(y=c|\\m{x}) &=\\frac{\\exp(z_c)}{\\sum_{c=1}^K \\exp(z_c)}\\\\\n","&= \\frac{\\exp(z_c-z)}{\\sum_{c=1}^K \\exp(z_c-z)}\n","\\end{align}\n","where $z=\\max_c z_c$. Here we subtract $\\max_c z_c$ from the log probabilities for better numerical stability.\n","\n","The `predictProbs(X)` function takes in a matrix $X$ of $N$ instances and outputs an $N\\times K$ matrix of posterior\n","probabilities. Each row $i$ of the returned matrix represents the posterior probability distribution over the $K$\n","classes for the $i$-th training instance. (Note that each row of the returned matrix will sum to 1.)"],"metadata":{"id":"ijPslrff5o3D"}},{"cell_type":"code","source":["class NaiveBayes:\n","\n","    def __init__(self, useAddOneSmoothing=True, alpha=1):\n","        \"\"\"\n","        Constructor\n","        \"\"\"\n","        self.AOS = useAddOneSmoothing\n","        self.smoother = alpha\n","\n","    def prior(self):\n","        \"\"\"\n","        Solves for the priors \\pi = P(y)\n","        Arguments:\n","            None\n","        Returns:\n","            \\pi, a vector of size num_classes\n","        \"\"\"\n","        p = np.zeros((self.num_classes,))\n","        for i in self.y:\n","            p[i] += 1\n","        return p/self.samples\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Trains the model\n","        Arguments:\n","            X is a n-by-d numpy array\n","            y is an n-dimensional numpy array\n","        \"\"\"\n","\n","        ### baseline attributes and priors\n","        self.y = y\n","        self.samples, self.features = X.shape # m,d\n","        self.classes = np.unique(y)\n","        self.num_classes = self.classes.shape[0]\n","        self.priors = self.prior()\n","\n","\n","        ### distinct feature values per column\n","        self.unique_features = []\n","        for d in range(self.features):\n","            self.unique_features.append(np.unique(X[:,d]))\n","\n","\n","        ### counts\n","        self.N_cd = np.zeros((self.num_classes, self.features)) # count of feature i per c\n","        self.N_c = np.zeros((self.num_classes)) # total count of feautures per c\n","\n","        for c in self.classes: # for each available y = c\n","\n","            # get indices where class is c\n","            indices = []\n","            for i in range(self.samples):\n","                if c == self.y[i]:\n","                    indices.append(i)\n","\n","            # get values of features where y = c\n","            temp_count = []\n","            for d in range(self.features):\n","                temp_count.append(np.sum(X[indices,d]))\n","\n","            # adding to N_ci and N_c\n","            self.N_cd[c] = temp_count\n","            self.N_c[c] = np.sum(temp_count)\n","\n","\n","    def prod_theta(self, x_i, c):\n","        \"\"\"\n","        Solves for theta_c = P(x_i|y=c) = P(x_0|y=c)*P(x_1|y=c)*...*P(x_D|y=c)\n","        Arguments:\n","            x_i is a sample of size self.features\n","        Returns:\n","            theta_c, an integer\n","        \"\"\"\n","        # Add One Smoothing\n","        if self.AOS:\n","            self.smoother = 1\n","        else:\n","            self.smoother = 0\n","\n","        value = 1\n","        for d in range(self.features):\n","            numer = self.N_cd[c,d] + self.smoother\n","            denom = self.N_c[c] + self.smoother*self.features\n","            value = value * ((numer/denom)**x_i[d])\n","        return value\n","\n","    def log_theta(self, x_i, c):\n","        \"\"\"\n","        Solves for log(theta_c) = log(P(x_i|y=c)) = log(P(x_0|y=c))+log(P(x_1|y=c))+...+log(P(x_D|y=c))\n","        Arguments:\n","            x_i is a sample of size self.features\n","        Returns:\n","            log(theta_c), an integer\n","        \"\"\"\n","        if not self.AOS:\n","            self.smoother = 0\n","\n","        value = 1\n","        for d in range(self.features):\n","            numer = self.N_cd[c,d] + self.smoother\n","            denom = self.N_c[c] + self.smoother*self.features\n","            value += np.log(((numer/denom)**x_i[d]))\n","        return value\n","\n","    def predict(self, X, logLikelyhood=True):\n","        \"\"\"\n","        Used the model to predict values for each instance in X\n","        Arguments:\n","            X is a n-by-d numpy array\n","        Returns:\n","            an n-dimensional numpy array of the predictions\n","        \"\"\"\n","        ### prediction matrix y_hat\n","        test_samples = X.shape[0]\n","        self.prediction = np.zeros((test_samples,))\n","\n","        ### calculating predictions for each sample x_i\n","        for i in range(test_samples):\n","            temp_likelyhood = np.zeros((self.num_classes,))\n","\n","            # for logarithmic needs\n","            if logLikelyhood:\n","                for c in range(self.num_classes):\n","                    temp_likelyhood[c] = np.log(self.priors[c]) + self.log_theta(X[i,:], c)\n","            else:\n","                for c in range(self.num_classes):\n","                    temp_likelyhood[c] = self.priors[c]*self.prod_theta(X[i,:], c)\n","\n","            # finding the most likely class\n","            max_index = np.argmax(temp_likelyhood)\n","            self.prediction[i] = self.classes[max_index]\n","\n","        return self.prediction\n","\n","\n","    def predictProbs(self, X, logLikelyhood=True):\n","        \"\"\"\n","        Used the model to predict a vector of class probabilities for each instance in X\n","        Arguments:\n","            X is a n-by-d numpy array\n","        Returns:\n","            an n-by-C numpy array of the predicted class probabilities (for C classes)\n","        \"\"\"\n","        ### prediction matrix y_hat\n","        test_samples = X.shape[0]\n","        self.probabilities = np.zeros((test_samples, self.num_classes))\n","\n","        ### calculating predictions for each sample x_i\n","        for i in range(test_samples):\n","            # for logarithmic needs\n","            if logLikelyhood:\n","                for c in range(self.num_classes):\n","                    self.probabilities[i,c] = np.log(self.priors[c]) + self.log_theta(X[i,:], c)\n","            else:\n","                for c in range(self.num_classes):\n","                    self.probabilities[i,c] = self.priors[c]*self.prod_theta(X[i,:], c)\n","\n","        return self.probabilities\n","\n","    def score(self, y):\n","        \"\"\"\n","        Finds the accuracy of the predictions\n","        Arguments:\n","            y is a np array of size n, likely a testing set\n","        \"\"\"\n","        n = len(self.prediction)\n","        error = self.prediction-y\n","        count = 0\n","        for i in error:\n","            if i == 0:\n","                count+=1\n","        return count/n"],"metadata":{"id":"ak2RjdCV56Lv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#from sklearn import datasets\n","from sklearn.metrics import accuracy_score\n","\n","# load the data set\n","dataset = datasets.load_digits()\n","X = dataset.data\n","y = dataset.target\n","\n","n, d = X.shape\n","nTrain = int(0.5*n)  # training on 50% of the data\n","\n","# shuffle the data\n","idx = np.arange(n)\n","np.random.seed(13)\n","np.random.shuffle(idx)\n","X = X[idx]\n","y = y[idx]\n","\n","# split the data\n","Xtrain = X[:nTrain, :]\n","ytrain = y[:nTrain]\n","Xtest = X[nTrain:, :]\n","ytest = y[nTrain:]\n","\n","print(Xtrain.shape)\n","print(ytrain.shape)\n","print(Xtest.shape)\n","print(ytest.shape)\n"],"metadata":{"id":"fUTthq6858xS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730438007537,"user_tz":240,"elapsed":148,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"844130b6-2eaf-43a7-99ff-2805b4f2bbe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(898, 64)\n","(898,)\n","(899, 64)\n","(899,)\n"]}]},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB # as a check of my implimentation\n","\"\"\"\n","SKLEARN naiveBayes package\n","\"\"\"\n","\n","sk_bayes = MultinomialNB()\n","sk_bayes.fit(Xtrain, ytrain)\n","\n","print(\"Accuracy percent of sklearn Multinomial NBC is:\", round(sk_bayes.score(Xtest, ytest),7))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3SqtuLgJIIxU","executionInfo":{"status":"ok","timestamp":1730438010237,"user_tz":240,"elapsed":161,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"1aed09e3-e2c5-4b7b-b8b9-9ab43bfecfc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy percent of sklearn Multinomial NBC is: 0.89099\n"]}]},{"cell_type":"code","source":["\"\"\"\n","My implimentation\n","\"\"\"\n","\n","# train the naive Bayes\n","modelNB = NaiveBayes(useAddOneSmoothing=True)\n","modelNB.fit(Xtrain, ytrain)\n","\n","# output predictions on the remaining data\n","ypred_NB = modelNB.predict(Xtest)\n","\n","# calculate the posterior probability\n","yposterior_NB = modelNB.predictProbs(Xtest)\n","\n","print(\"Accuracy percent of my implimentation of Multinomial NBC is:\", round(modelNB.score(ytest),7))\n"],"metadata":{"id":"57gRg93hIPyQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730438042992,"user_tz":240,"elapsed":2375,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"442c3d93-a4a1-4b88-d9b5-33f3e596e1f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy percent of my implimentation of Multinomial NBC is: 0.89099\n"]}]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n","\n","# Q3: Decision Trees (30pt)\n","\n","In this question we are going to focus on decision trees, manually follow the algorithm, and investigate the decisions the algorithm makes. Our goal is to justify the steps of the algorithm and investigate the redundancy in the space of decision trees. Consider the following decision tree:\n","\n","\n","<img src=\"https://github.com/yexf308/MachineLearning/blob/main/homework/HW3/Decision_tree1.png?raw=true\" width=\"400\" />\n","\n","## Q3.1 Decision Boundaries  (5pt)\n"," Draw the decision boundaries defined by this tree. Each leaf of the tree is labeled with a letter. Write this letter in the corresponding region of instance space and label the axes. Your solution should look like Next Figure, where you will replace the boundary thresholds and labels based on the provided decision tree.\n","\n","  <img src=\"https://github.com/yexf308/MachineLearning/blob/main/homework/HW3/Decision_tree2.png?raw=true\" width=\"400\" />"],"metadata":{"id":"Mcb8d-voArdU"}},{"cell_type":"code","source":["!['Decision Bounds'](https://drive.google.com/uc?export=view&id=1xhTudA_9F3P5jVgXVpZd1m1__jwpcKm1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DgvM4lR1eJKi","executionInfo":{"status":"ok","timestamp":1730477654086,"user_tz":240,"elapsed":280,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"e42e89a2-cbb6-4465-e1f6-beed64e7a87c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: -c: line 1: syntax error near unexpected token `https://drive.google.com/uc?export=view'\n","/bin/bash: -c: line 1: `['Decision Bounds'](https://drive.google.com/uc?export=view&id=1xhTudA_9F3P5jVgXVpZd1m1__jwpcKm1)'\n"]}]},{"cell_type":"markdown","source":["\n","---\n","\n","## Q3.2 Decision Boundaries  (5pt)\n","Give another decision tree that is different from decision tree in first Figure but defines the same decision boundaries. This demonstrates that the space of decision trees is syntactically redundant. Would this redundancy affect the accuracy of the learned trees? What are potential benefits of this redundancy? (i.e., Does it increase the computational complexity of finding an accurate tree?) Explain using 3-5 sentences."],"metadata":{"id":"EuMa8J3KOtY5"}},{"cell_type":"code","source":["![](https://drive.google.com/uc?export=view&id=1AABvopogEiAN6TgmbbItNFttP0hgvcLL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdwAHnGtfRNJ","executionInfo":{"status":"ok","timestamp":1730477485079,"user_tz":240,"elapsed":154,"user":{"displayName":"Brendan Premo","userId":"11086205363528817120"}},"outputId":"fb13034b-0ea8-4fcc-d908-8e58cb8efe9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: -c: line 1: syntax error near unexpected token `https://drive.google.com/uc?export=view'\n","/bin/bash: -c: line 1: `[](https://drive.google.com/uc?export=view&id=1AABvopogEiAN6TgmbbItNFttP0hgvcLL)'\n"]}]},{"cell_type":"markdown","source":["# Your solution:\n","\n","In this case, the trees only differ on when we decide to look at $x_2$, the first deciding after looking at $x_1$. The accuracy of either tree is only a matter of interpretation, and I would prefer to decide one feature at a time rather than reaccessing information about, say $x_1$, after already making a decision on it. Computationally, if we were to use the first tree as our model, we would be checking $x_1$ twice, where with proper implimentation and a three-leaf tree, we would only have to check each $x_1$ once.\n","\n","As for benefits to the redundancy, it could be the case that over half of the sample has $x_1>25$, thus it would be wise to check it first to decrease overall computation time. Either way, I don't believe (for this particular example) that the accuracy of the tree changes much in regards to either displayed tree."],"metadata":{"id":"Uik_Ax2jOyBP"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Q3.3 Building A Decision Tree (5pt)\n","Consider the training samples in next Figure, where A, B, C denote three different features and Y denotes the output we want to predict:\n","\n","\n","<img src=\"https://github.com/yexf308/MachineLearning/blob/main/homework/HW3/Decision_tree3.png?raw=true\" width=\"200\" />\n","\n","What feature would be chosen for the split at the root of a decision tree using the Information Gain criterion? Please show your calculations and explain your reasoning. What is the relationship between the selected feature and outcome Y?"],"metadata":{"id":"ngAKGOtqO41p"}},{"cell_type":"markdown","source":["# Your solution:\n","\n","We pick the feature with the highest information gain, with respect to ID3, which turns out to be feature $C$. From a glance, we can see $C$ and $Y$ have an inverse relationship.\n","\n","$$\n","\\begin{align*}\n","H(\\mathcal{S}) &= -\\sum_{i=1}^k p_i\\log_2 p_i\\\\\n","&= -\\frac{3}{6}\\log_2\\frac{3}{6} - \\frac{3}{6}\\log_2\\frac{3}{6}\\\\\n","&= \\frac{1}{2} + \\frac{1}{2}\\\\\n","&= 1\\\\\n","\\\\\n","H(\\mathcal{S}|A) &= \\sum_{a\\in A} \\frac{|\\mathcal{S}_a|}{|\\mathcal{S}|}H(\\mathcal{S}_a)\\\\\n","&= -\\frac{3}{6}*(\\frac{1}{3}\\log_2\\frac{1}{3}) - \\frac{3}{6}*(\\frac{2}{3}\\log_2\\frac{2}{3})\\\\\n","&≈ .459 \\\\\n","\\text{IG}(\\mathcal{S}, A) &= H(\\mathcal{S}) - H(\\mathcal{S}|A) = 1 - .459 = .541\\\\\n","\\\\\n","H(\\mathcal{S}|B) &= \\sum_{b\\in B} \\frac{|\\mathcal{S}_b|}{|\\mathcal{S}|}H(\\mathcal{S}_b)\\\\\n","&= -\\frac{4}{6}*(\\frac{2}{4}\\log_2\\frac{2}{4}) - \\frac{2}{6}*(\\frac{1}{2}\\log_2\\frac{1}{2})\\\\\n","&= .5 \\\\\n","\\text{IG}(\\mathcal{S}, B) &= H(\\mathcal{S}) - H(\\mathcal{S}|B) = 1 - .5 = .5\\\\\n","\\\\\n","H(\\mathcal{S}|C) &= \\sum_{c\\in C} \\frac{|\\mathcal{S}_c|}{|\\mathcal{S}|}H(\\mathcal{S}_c)\\\\\n","&= -\\frac{3}{6}*(\\frac{0}{3}\\log_2\\frac{0}{3}) - \\frac{3}{6}*(\\frac{3}{3}\\log_2\\frac{3}{3})\\\\\n","&= 0 \\\\\n","\\text{IG}(\\mathcal{S}, C) &= H(\\mathcal{S}) - H(\\mathcal{S}|C) = 1 - 0 = 1\\\\\n","\\end{align*}\n","$$"],"metadata":{"id":"i9-MBRZVPdZ5"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Q3.4 Random Splitting  (10pt)\n","In the basic decision tree algorithm, we choose the feature/value pair with the maximum Information Gain (i.e., $IG(Y |X ) = H (Y ) − H (Y |X ))$ as the criterion to use at each internal node of the decision tree. Suppose we modified the algorithm to choose at random from among those feature/value combinations that had non-zero information gain, but that we kept all other parts of the algorithm unchanged.\n","\n","Prove that if a splitting feature/value combination has non-zero information gain at an internal node, then at least one training example must be sent to each of the child nodes.\n","\n","**Hint:** You may prove the contrapositive of the statement instead, that is, if all examples are sent to one of the child nodes (they have the same feature/value pair) then the information gain is zero.\n"],"metadata":{"id":"B-U_1RsuPnk8"}},{"cell_type":"markdown","source":["# Your solution:\n","\n","We choose to prove the contrapositive. Let $\\mathcal{S} = \\{1,2,...,k\\}$ be samples and $F = \\{f_1, f_2, ..., f_{t}\\}$ be options for a feature $F$ such that, WLOG, $\\forall s\\in\\mathcal{S}, s\\in\\mathcal{S}_{f_1}.$ Thus, $\\mathcal{S}\\subset \\mathcal{S}_{f_1}$, but by definition, $\\mathcal{S}_{f_1}\\subset \\mathcal{S}$, so $\\mathcal{S} = \\mathcal{S}_{f_1}.$\n","\n","Then, we have the following:\n","$$\n","\\begin{align*}\n","H(\\mathcal{S}|F) &= \\sum_{f\\in F} \\frac{|\\mathcal{S}_f|}{|\\mathcal{S}|}H(\\mathcal{S}_f)\\\\\n","&= \\frac{|\\mathcal{S}_{f_1}|}{|\\mathcal{S}|}H(\\mathcal{S}_{f_1}) + \\frac{|\\mathcal{S}_{f_2}|}{|\\mathcal{S}|}H(\\mathcal{S}_{f_2}) + ... + \\frac{|\\mathcal{S}_{f_t}|}{|\\mathcal{S}|}H(\\mathcal{S}_{f_t})\\\\\n","&= \\frac{k}{k}H(\\mathcal{S}_{f_1}) + \\frac{0}{k}H(\\mathcal{S}_{f_2}) + ... + \\frac{0}{k}H(\\mathcal{S}_{f_t})\\\\\n","&= 1*H(\\mathcal{S}_{f_1}) + 0 + ... + 0\\\\\n","&= H(\\mathcal{S}_{f_1})\\\\\n","&= H(\\mathcal{S})\\\\\n","\\\\\n","\\text{IG}(\\mathcal{S}; F) &= H(\\mathcal{S})- H(\\mathcal{S}|F)\\\\\n","&= H(\\mathcal{S})- H(\\mathcal{S})\\\\\n","&= 0 \\\\\n","\\blacksquare\n","\\end{align*}\n","$$\n","\n","This is equivalent to the above statement that if a splitting feature/value combination has non-zero information gain at an internal node, then at least one training example must be sent to each of the child nodes."],"metadata":{"id":"sMV4ZRWvP3jF"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","## Q3.5 Random Splitting (5pt)\n","How do you think this change (i.e., choosing at random from among those feature/value combinations that have non-zero information gain) would affect the accuracy of the decision trees produced on average? To achieve the same accuracy, how are the trees different (e.g. size) on average? Why? Explain using 3-5 sentences."],"metadata":{"id":"8k2faCoaP_KV"}},{"cell_type":"markdown","source":["# Your solution:\n","\n","Since there is no sense of quantifying which features are important, we must sort through each and every choice of each feature before we can be certain that we have finished the tree. Because of this, we will be accurate, but have massive amounts of overfitting and, if there are $k$ features $F$, where $F_i = \\{ f_1, f_2,..., f_{j_i}\\}$, then we will have a height of at least $k$ and number of decisions equal to $\\prod_{i=1}^k|F_i|-1$, which can get quite large if $k$ and/or the $j_i$'s become large and become a bore to understand at a glance.\n","\n","In other words, without a stopping threshold, we will have an extremely large runtime and have massive overfitting issues (massive tree size, etc.), but with a stopping threshold, we have the potential to lose out on accuracy if we don't randomly select any high information-gain features, which cannot be guarenteed. The same can be done for a ID3 derived tree by just decreasing the stopping threshold with a guarentee of getting high information-gain features in the tree first, leading to a more informative tree altogether."],"metadata":{"id":"eiIi8cNrQGN8"}}]}